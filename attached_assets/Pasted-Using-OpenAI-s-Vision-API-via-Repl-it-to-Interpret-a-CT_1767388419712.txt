Using OpenAI’s Vision API via Repl.it to Interpret a CT Scan Video

Introduction

Example of a CT scan frame (axial chest slice). With OpenAI’s multimodal API, such medical images can be analyzed and described in natural language for educational purposes.

Leveraging OpenAI’s API on Repl.it to analyze a short CT scan video can provide a step-by-step radiology interpretation for a new learner. In this approach, a 10–15 second screen recording of scrolling through CT slices is broken into image frames and fed to an OpenAI vision-capable model (like GPT‑4 Vision or GPT‑5.2). The AI can then describe anatomical structures and highlight key findings across those frames, simulating how a radiologist “reads” the scan. This can be a powerful educational tool, but we must handle the video input carefully and be mindful of the model’s limitations (especially since it’s not a specialized medical expert ￼).

Challenges and Considerations

Interpreting CT scans with a general-purpose AI model presents some challenges and important considerations:
	•	Medical Image Complexity: CT scans are specialized medical images, essentially 3D data (multiple slice images). OpenAI’s models are not medically trained radiologists, so their interpretations might miss subtle findings or even be incorrect. In fact, OpenAI’s documentation notes that the vision model “is not suitable for interpreting specialized medical images like CT scans and shouldn’t be used for medical advice” ￼. Therefore, any interpretation should be used only for educational guidance with proper disclaimers, not actual diagnosis.
	•	Video Input vs Image Input: The OpenAI API does not accept video files directly for analysis. Instead, we need to extract images (frames) from the video. A CT scan video typically shows a series of slice images as one scrolls through the scan. We must convert this into individual image frames that the AI can process. Research in medical AI suggests treating 3D imaging data (CT/MRI) as a sequence of frames (like a video) to leverage vision-language models effectively ￼. We will apply this concept by taking representative frames from the CT video for the model to analyze.
	•	Ensuring Accuracy: Since the goal is the “most accurate answers”, it’s important to present the model with clear, high-quality images and sufficient context. This might mean selecting key frames (especially where an important finding is visible) and using the API’s high detail mode for image analysis. We should also craft prompts that focus the AI on describing what’s actually in the images (to minimize guesswork or hallucination ￼). Remember that the model may still make mistakes or overly general statements about the CT – it’s not infallible, so the output should be reviewed by someone with medical knowledge.

Extracting Frames from the CT Video

The first technical step is to break the 10–15 second CT scan video into image frames. On Repl.it, you can use tools like FFmpeg or OpenCV in a Python Repl to accomplish this. For example:
	1.	Upload the Video: Add the CT scan clip (e.g. ct_scan.mp4) to your Repl.it project. Ensure it’s a short clip (a few seconds) to keep processing quick and within API limits.
	2.	Extract Key Frames: Use FFmpeg or OpenCV to sample frames from the video. You don’t need every single frame (which could be hundreds); instead, capture maybe one frame per second or a handful of representative slices. For instance, with FFmpeg you could run a command to get 10 frames from the 10s video (one per second). Or with Python OpenCV, read the video and grab frames at intervals.
	•	Another approach is to extract ~8–12 frames evenly spaced through the scroll. This will cover the scan from start to end. Make sure the frames are in order (from the top of the scan to bottom, or vice versa).
	3.	Optional – Create a Frame Collage: To simplify sending multiple frames to the AI, you can combine them into a single image (a tiled grid). A community experiment found that GPT-4 Vision can interpret a sequence of video frames if they are presented together in one image ￼. For example, you might create a 3x3 grid image containing 9 key frames in sequence. This “photo grid” method compresses the video’s story into one image file. If you do this, consider adding small labels or an order (though GPT-4 can often infer sequence if frames are arranged logically).
	4.	Save Frames: Ensure each extracted frame or the collage is saved as a JPEG/PNG image. Keep file sizes reasonable (the OpenAI API accepts images up to 20 MB each, and a total of 50 MB per request ￼). CT images are usually grayscale and not huge in resolution, so this is typically fine. If using a collage, you might reduce each frame’s size so the combined image isn’t too large.

By the end of this step, you will have either a set of image files (e.g. frame1.jpg, frame2.jpg, …) or a single combined image (e.g. ct_grid.jpg) representing the video. These are what we’ll feed into the OpenAI API.

Integrating OpenAI’s Vision API on Repl.it

Once we have the CT images, the next step is calling OpenAI’s API to analyze them. Here’s how to integrate and use the API on Repl.it:
	•	Setup OpenAI API Key: In your Repl.it, go to the “Secrets” or environment variables section (as shown in Replit’s guide) and add your OPENAI_API_KEY ￼. This keeps your key secure. In your code, load this key (e.g. via os.environ["OPENAI_API_KEY"]) and initialize the OpenAI client (using the openai Python library or HTTP calls).
	•	Choose the Right Model: Select a model that supports image inputs. In 2024 onward, GPT-4 Vision (sometimes called GPT-4o) was the flagship vision model ￼. By 2025, GPT-5.2 also has multimodal capabilities and can “perceive images” ￼. For example, using the OpenAI Python library you might specify model="gpt-4-vision" or the appropriate model name (OpenAI’s documentation or model card will confirm the exact identifier; GPT-5.2 likely has vision enabled by default given its improvements in image understanding ￼).
	•	Sending Images in the API Call: The OpenAI chat completion API allows including images alongside the prompt. You cannot directly attach raw image files in a plain REST call; instead, you have two main options:
	1.	Upload and reference via File ID: Use the OpenAI Files API to upload each image, obtaining a file_id for each ￼ ￼. Then your message content can include an object like {"type": "input_image", "file_id": "<your_file_id>"} for each image.
	2.	Encode as Base64 Data URL: Read the image file in Python, base64-encode it, and prefix with data:image/jpeg;base64,. This string can be placed in the API JSON as the image_url. For example, the content could include {"type": "input_image", "image_url": "data:image/jpeg;base64,<encoded_data>"} ￼ ￼. (If using the OpenAI Python SDK, it might handle some of this for you under the hood when you pass an image path.)
The OpenAI Responses API (the newer chat completion interface) can handle multiple image inputs in one request, processing each and combining information ￼. So you can include all your frame images in a single prompt message. According to OpenAI, the model will look at each image and use info from all of them to answer your question ￼. This is perfect for a CT sequence, since the AI can see the progression of slices. Make sure the images are in the intended order (the model doesn’t inherently know which frame is first or last unless you tell it or arrange them clearly). If you made a collage image, then you’re just sending that one image (which inherently shows the sequence). If you send multiple separate images, consider adding brief text cues in the prompt like “Image 1 is the top slice, Image 2 is the next slice…”. This can help the model understand the temporal/physical order of the slices ￼.
	•	Setting Image Detail Level: You may want the model to give a very thorough look at each CT image. OpenAI’s API supports a detail parameter for images ("low", "high", or "auto"). Using "detail": "high" ensures the model processes the image at high resolution and spends more “tokens” analyzing fine details ￼. For medical images, this is likely beneficial to catch subtle structures. The trade-off is cost and speed, but since our frames are limited, high detail is recommended to maximize accuracy.
	•	Constructing the Prompt: Now, form the messages for the chat completion:
	•	Include a system message to set context, for example: “You are an expert radiologist explaining a CT scan to a medical trainee. You will be given a series of CT images and you should provide an interpretation, describe important findings or ‘signs’, and explain what they mean in simple terms.” This primes the AI to give an educational, explanatory answer.
	•	Then the user message will contain the images and a question/instruction. For instance, the user content (in JSON) might look like:

"role": "user",
"content": [
   { "type": "input_text", "text": "Below are several CT scan slices from a short video. Please interpret what you see for an educational purpose. Describe any notable findings and identify key anatomical structures visible in each frame." },
   { "type": "input_image", "image_url": "<data URL of frame 1>", "detail": "high" },
   { "type": "input_image", "image_url": "<data URL of frame 2>", "detail": "high" },
   ... (more images) ...
 ]

If using a single collage image, you’d just have one "input_image". In the text prompt, you can mention that it’s a sequence of frames (e.g. “The following image is a grid of CT scan frames showing a scan from top to bottom of the chest…”). This aligns with strategies others have used – e.g., one prompt said “The image shows video frames in sequence. Describe what’s likely going on in each frame.” ￼, which helped GPT-4 Vision narrate a frame-by-frame analysis.

	•	Make the API Call: Using the OpenAI Python library, you would call something like openai.ChatCompletion.create(...) with the model, and your messages array containing the system and user message (with images embedded as shown). On Repl.it, simply run this code. The first time, the model may take a handful of seconds to process the images (especially with high detail). Ensure you handle any errors (for example, if the payload is too large or if any image failed to upload).
	•	Receive and Parse the Response: The API will return a JSON with the assistant’s answer in text. You can print this to the console or format it for display. The answer should ideally describe the CT scan. For example, it might say: “Frame 1 (upper chest): The scan shows the upper lungs and part of the trachea… Frame 5 (middle chest): the heart (white region in the center) is visible, and the lung fields are dark indicating air. No obvious lesions or nodules are seen. The costophrenic angles are clear…,” and so on – along with explanatory notes. Because we instructed it to be educational, it might also explain what certain normal structures are, and highlight any “signs” (if there were, say, an abnormal finding like a tumor or an area of pneumonia, the model might point it out if it recognizes it). Keep in mind the AI might also say it’s not a medical device or provide a caution, given the sensitive domain.
	•	Iterate if Needed: If the output seems incomplete or the model didn’t use all frames effectively, you can refine the prompt. Sometimes it helps to explicitly ask for a frame-by-frame description or a summary of overall findings. Since accuracy is a priority, double-check if the AI’s statements align with known CT anatomy. You might need to prompt it to be more specific (e.g., “Identify the lungs, heart, bones, and any abnormal areas in each frame.”). Because the model might be unsure, using language like “likely” or “appears” in the prompt can allow it to hypothesize without triggering refusals ￼.

Prompting the Model to “Show Signs”

The user’s goal is educational interpretation, “showing signs”, and teaching how to read the CT. Achieving this is as much about prompt design as it is about the technical integration. Here are some tips for crafting the prompt to get a useful teaching output:
	•	Ask for an Interpretation and Key Findings: Be direct in your request. For example: “Explain the CT scan as if teaching a student. What does each image slice show? Point out any abnormal findings or interesting signs.” This invites the model to go slice by slice and also mention any pathology or notable normal variants. If the CT is normal and you want it to still teach, you can say “if everything looks normal, describe the normal anatomy visible.”
	•	Use the Language of Education: Phrases like “for educational purposes” or “to a new learner” encourage the model to elaborate and clarify. You might get a more thorough explanation with simpler language and definitions of terms. For instance, the model might explain “these black areas are the lungs filled with air – that’s normal” or “the heart is the large white structure in the middle; the bright white around it is contrast or dense tissue like bone”.
	•	Incorporate Known Radiology Terms (Carefully): If you expect certain classic signs (like “ground-glass opacities” or “pleural effusion” etc.), you could mention in the prompt: “Identify any radiological signs (e.g. consolidations, nodules, fluid levels) if present.” However, be cautious: if the model isn’t certain, it might hallucinate a finding just to comply. It’s often safer to ask it to describe what it sees and then infer. For example: “Describe any abnormal patterns you see in the lung fields or other organs.” This way, it will stick to describing the image, which you (or an expert) can interpret.
	•	Limit the Scope if Necessary: If the CT video is of a specific region (say just the chest), clarify that in the prompt so the model doesn’t start talking about other body parts. E.g., “These images are axial slices through the chest (thorax). Focus on the lungs, heart, and chest anatomy.” This focuses the answer on relevant structures.

By guiding the model with a well-constructed prompt, you increase the chance of an accurate, useful interpretation. The output can then be reviewed, and any corrections or additional context can be provided to the learner by a human if needed.

Implementation Steps on Repl.it (Summary)

Putting it all together, here’s a step-by-step summary of how you would implement this on Repl.it:
	1.	Prepare the Environment: Create a new Repl (Python is a good choice). Add your OpenAI API key to Repl.it Secrets (as OPENAI_API_KEY). Install any needed libraries (openai, ffmpeg, opencv-python, etc.) via the Repl package manager.
	2.	Upload CT Video: Add the CT scan short video file to the Repl.
	3.	Extract Frames (Code): Use a Python script to extract a set of image frames from the video. For example, use OpenCV:

import cv2
vid = cv2.VideoCapture('ct_scan.mp4')
count = 0
frames = []
while vid.isOpened():
    ret, frame = vid.read()
    if not ret: break
    if count % 30 == 0:  # grab one frame per 30 frames (~1 sec if video 30fps)
        cv2.imwrite(f'frame{count}.jpg', frame)
        frames.append(f'frame{count}.jpg')
    count += 1
vid.release()

This saves frames at interval. Adjust logic to get the desired number of slices (or simply stop after N frames). Alternatively, call ffmpeg in a subprocess to export a set number of frames. Ensure frames are in correct anatomical order (you might need to reverse the list depending on how the video was recorded).

	4.	(Optional) Create a Grid Image: If you prefer a single image input, you can programmatically stitch the frames together using PIL or OpenCV. For instance, create a blank canvas and paste each frame thumbnail in sequence. Save this as grid.jpg.
	5.	Call the OpenAI API: Use the openai library in Python to call the chat completion. Construct the messages as described earlier. For multiple images, you can either upload each via openai.File.create() and use file IDs, or read and base64-encode as data URLs. Using the data URL approach:

import openai, base64
openai.api_key = os.environ["OPENAI_API_KEY"]
# Construct system and user messages
messages = [
  {"role": "system", "content": "You are an expert radiologist... (system prompt)"},
  {"role": "user", "content": [
       {"type": "input_text", "text": "Please interpret these CT scan images... (user prompt)"},
       # include either multiple frames or one grid:
       *[{"type": "input_image", "image_url": f"data:image/jpeg;base64,{base64.b64encode(open(f, 'rb').read()).decode()}", "detail": "high"} for f in frames]
   ]}
]
response = openai.ChatCompletion.create(model="gpt-4-vision", messages=messages)
result_text = response['choices'][0]['message']['content']
print(result_text)

(The above pseudo-code loops through each image path in frames and appends an image content. Adjust model name as needed, e.g., GPT-5.2 if that’s the target and it supports the same input format.)

	6.	Review the Output: The AI’s answer will appear in the console. It should describe the CT slices and give an interpretation. Read it and verify that it makes sense. Remember, the model might include a caveat like it’s not a diagnostic system – that’s actually a good thing to keep for educational framing. If some parts are incorrect or unclear, you can tweak the prompt and run again. The cost will depend on image tokenization, but a handful of images is usually manageable (OpenAI’s pricing calculator can estimate this ￼).
	7.	Refine or Present the Results: Finally, you can format the output nicely, maybe printing each frame’s description on a new line or turning it into a small report. On Repl.it, you could even build a simple web interface to upload a video and display the AI-generated interpretation. But at its simplest, the console output itself can be copied and used as the educational explanation of the CT scan.

By following these steps, you effectively walk Repl.it through using OpenAI’s latest API features to analyze a CT scan video. The key technical components are extracting frames from the video and using the multimodal input capability of the OpenAI model to process multiple images in one go. This approach provides a “best possible simple” pipeline to have an AI read a CT, given current API capabilities.

Conclusion

Using Repl.it to interface with OpenAI’s vision-enabled models allows us to convert a CT scan video into a rich, educational narrative. The process involves frame extraction and careful prompt design, but does not require any complex model training – we rely on OpenAI’s pre-trained capabilities. While the OpenAI API (GPT-4/5 with vision) can generate a plausible interpretation of CT images, it’s important to keep expectations realistic. The model can describe obvious structures and maybe flag glaring abnormalities, but it won’t be as precise as a radiologist and should not be trusted for real medical decisions ￼. For teaching purposes, however, it can articulate what’s on the scan in simple terms, helping new learners understand how to navigate a CT. This pipeline demonstrates a cutting-edge use of multimodal AI: by treating a series of CT slices as a “video” input, we harness the model’s ability to synthesize information across multiple images ￼ ￼. The result is an approachable explanation of complex medical imagery, delivered through a straightforward Repl.it application using OpenAI’s API. With careful setup and prompt guidance, you can indeed continue using OpenAI’s models (like GPT-5.2) to read CT scans – just as you have for single images – now extended to sequences for a more complete and accurate picture.

Sources: The solution above integrates guidance from OpenAI’s official documentation on image inputs and limitations ￼ ￼, insights from community experiments on video frame analysis ￼, and research perspectives on applying video-text AI models to 3D medical images ￼ ￼. These references ensure that the approach follows current best practices for API integration and acknowledges the model’s constraints when dealing with CT scan data.